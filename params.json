{
  "name": "Spectral graph clustering",
  "tagline": "",
  "body": "# Project Proposal\r\n\r\n[Full PDF](https://github.com/feroze/spectral_graph_clustering/raw/gh-pages/15_618_Project_Proposal.pdf)\r\n\r\n## Summary\r\nWe intend to implement a hybrid MPI + CUDA algorithm to perform spectral clustering of an input graph. We will test the algorithm against a sequential MATLAB implementation and a parallel single-GPU CUDA implementation using one (or more) of the SNAP data sets on two fronts: quality and overall computation time. We may use more precise measurements to support our claims and verify our hypotheses.\r\n\r\n## Background\r\n\r\nGiven an undirected similarity graph , a clustering of the graph is an assignment of vertices  to clusters  such that a particular cost metric on the similarities  over the clusters  is minimized.\r\n\r\nA normalized cut of a graph is a partition of the graph into two partitions  and  such that the sum of the weights of the edges connecting vertices in  to those in  is the minimum of all possible partitions and the size of  and  are similar.\r\n\r\nSince this problem is NP-hard in nature, the approach of spectral clustering attempts to relax the constraints of the normalized cut and approximate the normalized cut using graph Laplacians.\r\n\r\nWithout diving into the mathematical details (see PDF for more details), the problem reduces to that of constructing the graph Laplacian (or the normalized graph Laplacian).\r\n\r\nAnd computing the two partitions using the second-smallest eigen value corresponding to one of the two Laplacian matrices. One popular method for computing the eigen values is using the Lanczos  algorithm, which involves iterative matrix-vector and vector-vector computations.\r\n\r\nTherefore, from an engineering standpoint, spectral graph clustering algorithms reduce to steps of matrix operations.\r\n\r\nSince GPUs are excellent candidates for speeding up matrix operations by performing independent computations in parallel, we intend to use them to speed up steps of the spectral clustering algorithm as well. Further, we intend to discover methods to distribute and perform these computations on a cluster of nodes using MPI, while continuing to exploit the parallelism offered by the GPUs on each of these nodes. The following figure illustrates our parallelization strategy.\r\n\r\n![](https://raw.githubusercontent.com/feroze/spectral_graph_clustering/gh-pages/parallel_approach.png)\r\n\r\nTo the best of our knowledge, this is the first such effort in this direction.\r\n\r\n## Challenges\r\n\r\n+ Spectral clustering is mathematically complex. It would take us time to understand the mathematics well enough to argue about the correctness of our results. This is not just from a purely mathematical perspective, but also from a systems point of view, as we are also limited by the numerical precision that our systems have to offer.\r\n+ Since we are using a hybrid parallelization approach, there will potentially be a lot of communication overhead. It is not easy to see why this might work until we have explored the best possible ways to reduce the communication overhead. We could potentially compute and store data that doesn't change once at the beginning of the algorithm, but we might then be limited by the available memory.\r\nFor a GPU on a single cluster node, we can view the data as being stored in a NUMA system:\r\nDevice Memory  Main Memory  Other Node's Main Memory  Other Node's Device Memory.\r\nWe might have to explore concepts such as pre-fetching/latency hiding/pipelining to improve performance.\r\n+ There are a lot of possible parallel approaches that we could explore. We could attempt to perform different steps of the algorithm in parallel, or we could operate in parallel on different parts of the data for the same algorithm step. For matrix operations, we could explore row-distributed representations, column-distributed representations, 2D-distributed representations, etc. We would have to narrow down on the best working approach fast enough to have enough time to refine it.\r\n+ Since we are attempting to get two different frameworks to work together, we might run into unforeseen issues in getting MPI and CUDA to work well together. We would have to work with a CUDA-aware version of MPI .\r\n+ If we are operating on sparse graph data sets, we would have to look into sparse matrix representations and tune our operations to perform well on these sparse representations. If data partitioning is not implemented correctly, we could potentially run into load balancing and gang skew issues as well.\r\n\r\n## Resources\r\nWe will be using a couple of papers to help us get familiar with the mathematical theory of spectral clustering, as well as sequential implementations of the same. We would also be referring to previous (non-hybrid) parallel implementations of parts of the algorithm, such as graph construction and eigen value computation.\r\n\r\n(See PDF for more details)\r\n\r\n## Goals and Deliverables\r\n\r\n### Plan to Achieve\r\n\r\nWe will have a working, correct implementation of a C++ hybrid MPI + CUDA parallel version of a Spectral Clustering algorithm as a miniature library. We expect the quality of the results produced by our algorithm to be comparable   to those produced by a sequential version of the algorithm.\r\n\r\nThe previous CUDA-only implementation  achieved 5x speedup over a sequential MATLAB version. Pessimistically, we expect to achieve at least 5x speedup over the sequential MATLAB version as well, but on larger data sets (as we have more total memory and compute power available to us).\r\n\r\nWe will also compare our code with MPI-only (if available) and CUDA-only parallel implementations of the algorithm, and hope to stay competitive.  We anticipate two major bottlenecks in our code: inter-node communication overhead in MPI and CUDA host memory to device memory transfer overhead. We will spend a significant amount of time optimizing and analyzing these two.\r\n\r\nFor the poster session, we will show a visualization of our speedup graphs, and discuss the parallelization approaches and trade-offs that we made. We will also show the interface to the library, the API structure and the easy of use of our code.\r\n\r\n###Hope to Achieve\r\n\r\nWe hope to achieve at least (5 + 0.5 * number of nodes)x speedup on two data sets. The reason is that we expect the algorithm to scale with the number of nodes as well, providing more speedup than the 5x speedup obtained using the CUDA-only implementation. However, we anticipate non-ideal speedup and limit ourselves to a more achievable goal of 0.5 * number of nodes.\r\n\r\nFurther, if our project goes better than expected, we will test against a few more data sets (with varying degrees of sparsity) and study the scaling characteristics of our code on these data sets. In the best case scenario, we would also perform isolated testing on AWS machines or on different GPUs to analyze the variability in our results.\r\n\r\n###In Case of Delays\r\n\r\nConversely, if our project does not go according to the time line due to unforeseen issues, we will implement a subpart of the algorithm completely and correctly, and describe exactly what extra work would need to be done to convert the partial results into the final expected result.\r\n\r\nIf our speedups are not as we expected, we will study the scaling properties of various parts of the algorithm individually and point out exactly why our algorithm does not work as we expected.\r\n\r\n## Platform Choice\r\n+    This project will be implemented using C++, CUDA and CUDA-aware MPI.\r\n+   Development will be done on local machines + GHC cluster machines. Debugging will also be done on the same.\r\n+   Our algorithm will be tested on the GHC cluster machines with GTX 1080s and OpenMPI. Our algorithm will not make any assumptions about the interconnect, but we will study time spent on communication between nodes.\r\n+   We may choose to use AWS machines or the Latedays cluster for isolated testing, if the variance in the obtained results is too high.\r\n\r\nOur code will be written from scratch in C++, but may choose to use Thrust/cuBLAS helper routines. We may study ideas from the previous CUDA-only versions to understand how they fit into our MPI + CUDA formulation of the algorithm, and decide to use a few of them (while providing due credit).\r\n\r\n## Schedule\r\n\r\n|Start Date | End Date | Work to be done |\r\n|---|---|---|\r\nNov 1 | Nov 5 | Understand the mathematics behind Spectral Clustering. Finalize an eigen decomposition algorithm. Finalize on a test data set. Compute benchmarks using existing implementations. |\r\nNov 6 | Nov 13 | Implement sequential Spectral Clustering. Compare against benchmarks.\r\nNov 14 | Nov 20 | Prepare the hybrid MPI + CUDA architecture. Develop subroutines for Spectral Clustering using CUDA. Distribute tasks across OpenMPI processes. Use small data sets to run correctness tests. |\r\nNov 21 | Nov 27 | Optimize the algorithm for large data sets. Work on Minimizing MPI/CUDA overheads. Run benchmark tests on large data sets. |\r\nNov 28 | Dec 4 | Buffer for pending tasks/running tests. |\r\nDec 5 | Dec 12 | Write the final project report. Prepare poster for presentation. Create necessary pending visualizations. |",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}