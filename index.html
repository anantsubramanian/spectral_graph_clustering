<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Spectral Graph Clustering using MPI + CUDA</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Spectral Graph Clustering</h1>
      <h2 class="project-tagline">Anant Subramanian &emsp; &emsp; &emsp; &emsp; Feroze Naina</h2>
      <a href="https://github.com/feroze/spectral_graph_clustering" class="btn">View on GitHub</a>
      <a href="https://github.com/feroze/spectral_graph_clustering/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/feroze/spectral_graph_clustering/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">

		<h2>
		The proposal for the project may be accessed <a id="proposal" class="anchor" href="proposal/index.html" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span>here</a>.</h2>
      <h1>
<a id="checkpoint" class="anchor" href="#checkpoint" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Checkpoint</h1>

		<h2>
		<a id="progress" class="anchor" href="#progress" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Progress Update</h2>

    <p>
    We have roughly followed the schedule that we had outlined in the project <a href="proposal/index.html">proposal</a>. We have shown this in the table in the later sections of this report. Following is the work we have done so far.
    </p>

    <ul>
      <li> We have finalized the test data sets from SNAP<a href="#Xsnapnets"><sup>[1]</sup></a>.  We are using a small 6,474 node, 13,233 edge undirected graph of Autonomous Systems for quick experimentation and correctness testing. This takes &lt; 1 minute for clustering using sequential code, making it ideal to run correctness tests. We also have a larger YouTube community graph with 1,134,890 nodes and 2,987,624 edges. This will be used to calculate the speedup w.r.t. the sequential versions. </li>

      <li> We spent a while to understand how the Lanczos algorithm would be implemented. We then designed strategies to implement this algorithm only on the GPU, and on the CPUs of nodes in a cluster only using MPI. We finally merged these two strategies to design a basic hybrid parallel version, distributed across nodes and GPUs. </li>

      <li> We have setup a test bench to run sequential versions of spectral clustering using MATLAB and numpy. We have obtained the timing for these on the small data set. The MATLAB time on Intel(R) Core(TM) i5-5300U CPU @ 2.30GHz for as20000102.txt (6,474 nodes, 13,233 edges) is 16.27 seconds. </li>

      <li> The MATLAB version performs K-means clustering and is used as the correctness benchmark for the Lanczos algorithm, as it is an approximate algorithm. We have chosen to divide data into 2 clusters for simplicity of testing. </li>

      <li> We have a basic CUDA + MPI hybrid version of the algorithm up to a certain stage<a href="#implementationNote"><sup>†</sup></a>. The algorithm distributes the data across the nodes of the cluster, in the device memories of the GPUs, and performs matrix operations on these partial views of the data. We have given an overview of this in the next section. </li>

    </ul>

<a id="implementationNote"></a><p><small><sup>†</sup>&ensp;We have managed to parallelize the implementation up to the Lanczos stage of the algorithm. We have highlighted the issues that we are facing beyond this point in the issues section.</small></p>

		<h2>
		<a id="hybridparallel" class="anchor" href="#hybridparallel" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Hybrid Parallel Approach</h2>

		<p>
    We are using the NVIDIA cuSparse <a href="#Xcusparse">[2]</a> library for node-local matrix operations that are performed on the GPU. This allows us to focus on tuning the performance of our library across nodes, while at the same time reaping the benefits of upstream updates to the cuSparse library. The goal is to produce a working library that is practically usable, with as little maintenance overhead as possible.
		</p>

		<p>
		Our algorithm can be broken down into three stages:
		<ol>
		<li> Construction of the Graph Laplacian from the input data set and distribution across nodes. </li>
		<li> Parallel Lanczos algorithm to compute the tri-diagonal matrix. </li>
		<li> QR algorithm to compute the Eigenvectors of the tri-diagonal matrix. </li>
		</ol>

		<p>
    Constructing the un-normalized Graph Laplacian from the distributed adjacency matrix is embarrassingly parallel. <b>Note that even a compressed matrix representation of the adjacency matrix can be easily converted into the compressed representation of the Laplacian.</b> We exploit this property to efficiently construct the Graph Laplacian.
    </p>

    <p>
    The following algorithm describes the working of the naïve hybrid MPI+CUDA code that we have at this stage for the Lanczos step of the algorithm.
    </p>

    <hr>
    <p><img src="hybrid_lanczos.png" alt=""></p>
    <hr>

    <p>
		Even though the naïve algorithm attempts to minimize communication be performing local matrix-vector operations in parallel across nodes, on the GPUs of each of the nodes, it still suffers from multiple issues. We have highlighted these, along with other issues, in the issues section.
		</p>

		<h2>
		<a id="issues" class="anchor" href="#issues" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Current Issues</h2>
		
		<ul>
      <li> One of the major issues we face is the parallel design of the QR algorithm as a post processing step of the Lanczos algorithm. The Lanczos algorithm constructs a tri-diagonal matrix to simplify the computation of Eigenvalues, but an algorithm like QR must be used to obtain the Eigenvectors of this tri-diagonal matrix. We had not accounted for this post-processing step in our project proposal. </li>
      <li> There is potential workload imbalance depending on how the Laplacian matrix L is distributed and stored. We have several strategies in mind to mitigate the issue of work load imbalance, that we intend to test in the coming weeks. </li>
			<li> There are potential numerical precision issues if we do not apply extra correction steps in the Lanczos algorithm. We may have to investigate correctness issues that may arise due to this. </li>
			<li> There seems to be a high communication overhead in the last allSumReduce operation of each Lanczos iteration, to compute <i>v</i><sub><i>j</i> +1</sub>. We may have to investigate ways to reduce this overhead. </li>
    </ul>

      <h1>
<a id="goals" class="anchor" href="#goals" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Goals and Deliverables</h1>

<p>
In light of the issues that we have discovered, we have slightly refined the goals from the original proposal.
</p>

<p>
We will still have a working, correct implementation of a C++ hybrid MPI + CUDA parallel version of a Lanczos-based spectral clustering algorithm as a miniature library. We expect the clustering quality of the results produced by our algorithm to be comparable to those produced by a sequential numpy version of the algorithm. For simplicity, we are restricting ourselves to the case of discovering 2 clusters.
</p>

<p>
The previous CUDA-only implementation <a href="#X418project">[3]</a> achieved <b>5x</b> speedup over a sequential MATLAB version. Pessimistically, we expect to achieve at least <b>5x</b> speedup over the sequential numpy version as well, but on larger data sets (as we have more total memory and compute power available to us).
</p>

<p>
We will also compare our code with the CUDA-only parallel implementations of the Lanczos algorithm, and hope to stay competitive.  We anticipate two major bottlenecks in our code: inter-node communication overhead in MPI and CUDA host memory to device memory transfer overhead. We will spend a significant amount of time optimizing and analyzing these two.
</p>

<p>
We hope to achieve at least <b>(5 + 0.5 * number of nodes)x</b> speedup on the two test data sets. The reason is that we expect the algorithm to scale with the number of nodes as well, providing more speedup than the 5x speedup obtained using the CUDA-only implementation. However, we anticipate non-ideal speedup and limit ourselves to a more achievable goal of 0.5 * number of nodes.
</p>

<p>
For the poster session, we will show a visualization of our speedup graphs, and discuss the parallelization approaches and trade-offs that we made. We will also show the interface to the library, the API structure and the easy of use of our code.
</p>

<h2>
<a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Revised Schedule</h2>

<table>
<thead>
<tr>
<th>Start Date</th>
<th>End Date</th>
<th>Work to be done</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nov 1</td>
<td>Nov 5</td>
<td><del>Understand the mathematics behind Spectral Clustering.</del> <del>Finalize an eigen decomposition algorithm.</del> <del>Finalize on a test data set.</del></td>
</tr>
<tr>
<td>Nov 6</td>
<td>Nov 13</td>
<td><del>Implement sequential Spectral Clustering. Compare against benchmarks.</del></td>
</tr>
<tr>
<td>Nov 14</td>
<td>Nov 20</td>
<td><del>Prepare the hybrid MPI + CUDA architecture.</del> <del>Write subroutines using the cuSPARSE library.</del> <del> Design and write the outline for the basic hybrid parallel version of the Lanczos algorithm.</del></td>
</tr>
<tr>
<td>Nov 21</td>
<td>Nov 24</td>
<td>Complete implementing the basic hybrid Lanczos by adding CUDA-aware MPI transfers. Use the small data set to run correctness tests. Implement the better data distribution strategy to counter workload imbalance.</td>
</tr>
<tr>
<td>Nov 25</td>
<td>Nov 27</td>
<td>Optimize the algorithm for large data sets. Work on Minimizing MPI/CUDA overheads.</td>
</tr>
<tr>
<td>Nov 28</td>
<td>Dec 1</td>
<td>Run benchmark tests on large data sets. Deal with QR stage of the algorithm. Find methods to perform fair evaluations.</td>
</tr>
<tr>
<td>Dec 1</td>
<td>Dec 4</td>
<td>Run tests using the fair evaluation scheme. Compute final benchmark numbers and design the graphs/visualizations to be used in the poster/writeup.</td>
</tr>
<tr>
<td>Dec 5</td>
<td>Dec 12</td>
<td>Buffer for pending tasks and tests. Write the final project report. Prepare poster for presentation. Create necessary pending visualizations.</td>
</tr>
</tbody>
</table>

<h2>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h2>

  <p class="noindent" >
  <div>
  <p><span>[1]<span>&#x00A0;&#x00A0;&#x00A0;</span></span>
  <a id="Xsnapnets"></a>Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection.
  <a href="http://snap.stanford.edu/data" class="url"><span class="pcrr7t-x-x-120">http://snap.stanford.edu/data</span></a>, June 2014.</p>

  <p class="noindent" >
  <div>
  <p><span>[2]<span>&#x00A0;&#x00A0;&#x00A0;</span></span>
  <a id="Xcusparse"></a>NVIDIA cuSPARSE::CUDA Toolkit Documentation.
  <a href="http://docs.nvidia.com/cuda/cusparse/" class="url"><span class="pcrr7t-x-x-120">http://docs.nvidia.com/cuda/cusparse/</span></a>, Accessed: 2016-11-20.</p>

  <p><span>
  [3]<span>&#x00A0;&#x00A0;&#x00A0;</span></span><a 
  id="X418project"></a>Parallel      eigensolver      for      graph      spectral      analysis      on      gpu.
  <a 
  href="http://linhr.me/15618/" class="url" ><span 
  class="pcrr7t-x-x-120">http://linhr.me/15618/</span></a>. Accessed: 2016-10-31.
  </p>
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/feroze/spectral_graph_clustering">Spectral graph clustering</a> is maintained by <a href="https://github.com/feroze">feroze</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>
  
  </body>
</html>
